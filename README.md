# Researchers-Recommendation
Developing a professional social network website to recommend valuable papers and potential researchers to users
Building website use front end tech like JSP and graph database like Neo4j
Applied graph based recommendation algorithm using personal Page-rank and Node2Vec

# Summer Research (Researcher Recommendation) Summary
**Goal and problems**
The Researcher Recommendation project aims to build a professional social network for researchers, helping researchers find potential partners and papers. The whole project can be transferred into 3 processes, finding and parsing researchers¡¯ information, competing different recommendation methods and showing the result. 
The 1st step need to handle the data from different data source and transferred that into different database. We need to retrieve data from DBLP in the form of XML file and TXT file using different tools. Besides, we need to parse and clean the given data. For the database, since different recommendation methods need different database, we need to learn not only the transaction database but also graph database.
The 2nd step need to use different recommendation methods. One method is to analyze researchers¡¯ context, while another is analyzing researchers¡¯ relationship. I focused more on analyzing researchers¡¯ relationship using graph database, using method such as Page-rank and Node2Vec. However, both methods are significantly time and memory consuming, we need to figure out how to solve the problem based on our current facility.
The 3rd step need to build the website, prepare the server, and combine all recommendation methods mentioned before. Since different methods were written in different programming language by different members, we need to figure out how to combine all our work together. Another problem is that some of the recommendation content is too much that exceed client¡¯s browser memory limitation, while other¡¯s is too less to satisfied users¡¯ need, we need to adjust the parameters in recommendation method to satisfied customers¡¯ need.
**Approaches and Progress**
Data wrangle and entry
The initial problem is how to retrieve data from DBLP. For the file in XML format, since XML file was built in a hierarchical-tag format, we used the ¡°element-tree¡± to find all elements of paper, author and conference. As we wanted to connect different entities together, we need to find the relationship between author, paper and conference.  The relationship can be concluded as ¡°Authorof¡± between paper and researcher, as ¡°PublishedAt¡± between paper and conference, and as ¡°Reference¡± between paper and paper. We used the ¡°Regex¡± to analyze data in the TXT file to find those relationships, used ¡°Split¡± to get the right element. We save the file in CSV file for future use.
The next problem is how to parse the data. The retrieved data was not all written in English, and that caused problem for data entry in the next step. We used ¡°Strip¡± to clean invalid data and ¡°Custom Entity¡± to fix invalid entity and solve the problems in foreign language recognition. 
After that, we tried to entry the data into the database. Since my part focused more on analyze researchers¡¯ relationship, I used graph database to store data. At that point, two graph database were available, one is the OrientDB, another is Neo4j. OrientDB stored data in the traditional transaction database and build the relationship between different entities used vector. Neo4j stored its data in two different entities, one for entity, another for relationship. In general, Neo4j could be easily applied and fit our demand, so we used it as our database. We need to learn the ¡°Cypher¡± language, a non-SQL language for database query, and use python to implement. We used ¡°Basic-auth¡± to connect python to Neo4j database and use ¡°Session.run¡± to apply ¡°Cypher¡±. 
However, we find each row need more than 0.01 second and it take huge amount of time for millions of data. In this case, we used the ¡°neo4j-import¡± tools to entry whole data in ¡°CSV¡± format. Since the query of relationship kept slow, we build a unique index to improve the capability. 
**Recommendation methods**
The first method we tried is Page-rank. To implement the Page-rank for recommendation purpose, we need to set the page rank for each entity to find their importance in the whole system, and then calculate the personal page rank for selected paper or researcher to recommend.
In the beginning, we use the dot product for the whole database to calculate the overall page rank. However, since there are millions entity and relationship, there would be trillion data in the matrix, which is both time and memory consuming. In this case, we used the sparse matrix to find only the influential entities. However, it still need us to set a threshold for how many times of the iteration, and it might have already converged to its limitation before the threshold. 
 In this case, we used ¡°pagerank_scipy¡± to do the page-rank with tolerance and maximum iteration times. It allowed the convergence process stop if the step was lower than tolerance or the iteration time exceed maximum. We set the tolerance small enough to distinguish different entities but not spend too much time. The whole process need to more than 6 hours. The overall page-rank time can be decreased using parallel computatoni, but since the DBLP data source updated monthly or quarterly, the 6 hours¡¯ page-rank updating can be tolerated. The overall page-rank can be used as a signal about how important the researcher is among all the researchers.
For the personal page-rank, we subtract those highly related entities for the selected entity, and then do the page-rank. The highly related papers meant reference between each papers, conference both papers published at. The highly related researchers meant researchers¡¯ papers were highly related. Less memory was needed compared to overall page-rank, however it still took too much. Moreover, the tolerance threshold set in personal page-rank cannot fit all conditions. Some professors might have tons of highly related researchers or papers, while others might have little. If we set the tolerance too high, those researchers who have little highly related entities might get nothing to recommend; if we set the tolerance too low, those professors who have a lot of highly related entities might exceed clients¡¯ browser memory limitation. 
We can eliminate that problem by adjusting the parameters. However, personal page-rank take half a minute to get the result. If we tried several parameters, it would take a while to get the right answer and it won¡¯t meet the commercial demand. In this case, the next step is to find method to decrease the time of personal page-rank.
The second method we tried is Node2vec. Node2vec is similar to word2vec, which use random walk to find the context for the target word, calculate the probability between the appearance of target word and words in context, and then build the model to predict whether the target word would appear when context appear. Node2vec build its context by random walking in subtract graph, and then get the vector for each entities. Node2vec need too much memory for our purpose and we used the server from university to solve the memory problem. The output of Node2vec was a vector, we use python and cypher to entry the data into our graph database. To do the recommendation, we can calculate the cosine distance between the entity selected and others in the subgraph. If 2 entities are highly related, the cosine distance of their Node2vec value would be close to 1. 
However, the parameters adjusting problem occurred in personal page-rank also appeared in node2vec. If we set the tolerance too high, those researchers who have little highly related entities might get nothing to recommend; if we set the tolerance too low, those professors who have a lot of highly related entities might exceed clients¡¯ browser memory limitation. Even Node2vec worked faster than page-rank, it still had to spend half minute to query the whole graph database to make the recommendation. We cannot take several parameters to test the best result for the node2vec. 
Website construction
Finished the recommendation methods, the next step is to build a website to display our result. Since different members of group worked with different language, we need to combine all programming language and platform together. We have tried to build the website using Flask based on python, however, it is hard to design the user interface. So we decided to divide front-end and back-end apart. We build the Back-end using Flask, which can receive Http request as input parameters and send the result as JSON file. We then build the Front-end using JSP with CSS, and connect the front-end to the back-end. In the next step, we would configure the server, such as improve the edition of Java, to implement our work.
**Result, Assessment and Future Plan**
As mentioned above, we have done almost all 3 processes during the summer¡¯s semester. The data parsing and entry part were successfully finished. As for both recommendation methods, we need to do further research on how to decrease the time.  The website construction part had been done and our further step is to configure the server. 

# Fall 2016 Research Summary (Researcher Recommendation)
**Goal and problems**
The Researcher Recommendation project aims to build a professional social network for researchers, helping researchers find potential partners and papers. The whole project can be transferred into 3 steps: finding and parsing researchers¡¯ information, recommending using different algorithms and showing the result with webpage. 
In Summer 2016, the recommendation system was built for the first two step, and in Fall 2016, the last step, website, was built to show the recommendation result. The website construction met 3 main problems: 1. What language and software should be used ;2. How to combine the website with the former recommendation; 3.  What other functional parts are needed.
There are several languages for website construction. Mostly developers build static website pages using HTML and JavaScript, which it is not functional enough for this project. In this one, the website was built with the dynamically generated web pages. The two main tools to build dynamically generated webpage were ASP.Net and JSP. The ASP.net can be used for Microsoft based server and the JSP can be used in Apache server. Since the group server used the Apache server, the website was built using JSP.
In the beginning, the webpage was designed using HTTP request. The recommendation system built in summer used Flask, a python script receive HTTP request as input parameters and send the result as JSON file. However, each recommendation took average 3 minutes and users would not wait so long for the result. In this case, we tried to save recommend result in Mysql database and redesign the webpage using JSP.
Besides recommendation parts, other functional parts such as user login, user profile edit and search other researcher were designed.
**Approaches and Progress**
Language used and software selected
The initial problem is what tools should be used for the website construction. HTML combined with CSS is enough for static website design, such as a blog page or a Wiki page. However, if we wanted to use SQL database or HTTP request for more functional parts, HTML is not enough. We used the dynamically generated web pages for this problem. The server-side dynamic webpage construction, such as ASP.Net and JSP, was control by application server, every new webpage assembled based on the different requests from users. A client-side dynamic web page processes like JavaScript would then parse the content that represents in the loaded webpage, which is free and available online. Different server system needs different server-side dynamic webpage construction tools. For Microsoft system, developers need to use IIS to launch the ASP.Net webpage, while Apache Linux need Tomcat to launch the JSP webpage. The group server used Apache, so we chose the JSP for the website design. 
Another problem is environment configuration. The Neo4j database and Tomcat need different Java version, need to set complex environment variables and edit a lot of configuration files. Besides, some of the JSP files need to import extra package into JRE library. Those process is the most time consuming part of the whole project.
Combined website with recommendation system
The recommendation system is built using Flask in Summer and updated in Fall. It parsed the data collected from DBLP, used Page-rank and Embedding algorithm for recommendation. Flask is a python script to provide recommendation based on users¡¯ HTTP request. User could set different parameters such as name, recommendation methods or numbers of recommendation to find the result they wanted. However, each recommendation took 3 minutes and the process follow the serial work flow. If several users sent request in the same time, it would take too much time to response. Besides, the system updated its recommendation monthly, the same user would not get a different recommendation within a month. In this case, we saved the recommended results in relational database like Mysql, and rebuilt the webpage. Due to the group server calculation capacity limit, we decided to select one thousand faculties in US top 10 Computer Science colleges as initial users recommended. 
Another purpose of the website is to get the feedback from users for the accuracy of recommendation. In order to provide users authorities to edit those recommended results, the database need to contain two schemas, one called Dblp for recommendation results, another called Demo for users¡¯ preference. We transferred the recommendation results from JSON to SQL like transaction data in Dblp schema. Users can verify the data provide, and their operation would save in the relational data in Demo schema. 
Website Function design
The whole website contains 5 main functions: Search, Login, Home, Edit and Suggest. Search page used auto-complete to for users to find potential co-workers or papers. 
Since most researchers use common name, it would be hard to distinguish which one users might actually interested in. In this case, auto-complete would suggest similar names to what users type and help them find what they really want.  The autocomplete use the prefix query that decrease the potential answers within 1,000 after 5 words, which can also avoid wrong query for the time consuming recommendation part. The User interface design for search page used the Kendo framework, make it easier for auto-complete presentation.
The login page gave the users the authority to login their homepage, prevent others manipulated their profile pages. It just used the basic Form-submit to get it works, which need further research for safety issues. Potential Methods could be using Sessions or Cookies to prevent direct HTTP request, or using verification code to prevent attack. User interface for login page use the Bootstrap Samples, contain missing parts reminder, wrong email format suggestion and wrong password suggestion. As mentioned before, the recommendation system is so time consuming that we only cover the first 1,000 top 10 computer science college faculties result. The login page can also help us find active users and got recommendation for those users after they login their home page.
The home page is a blog page. The home page has two versions, one for presentation, one for editing. If one user wanted to find precise information of another user, he could search in the search pages and then jump to the other¡¯s home page, find his affiliation, research fields, and publications. The searcher can also find the other¡¯s interested researchers or papers generated by recommendation system. In the view of homepage owner, he can easily check and adjust the information like publications on his home page, make a judgement to whether the recommendation he interested in or not.  The home page also used Bootstrap for the framework design, and make different designs for biology, publications, papers recommendation and researchers¡¯ recommendation parts.  The framework design contains navigation bar, button format and table format, the biology design contains image format. For publications and papers recommendation, it used a collapsed design to avoid the publication history occupy most of the page. For researcher¡¯s recommendation, it used owl Carousal to show the users 4 potential co-workers at one time. The home page design also fit the mobile explorers.  
The editing pages contain two main part, the Profile editing page and Recommendation editing page. User can update or delete their information in the profile editing page, and choose the recommendation they interest. The editing page fun¡¯s function is based on SQL sentence such as Update, Delete and Insert on two schema of database, one called DBLP for suggestion and another called demo to keep track of user action. If user¡¯s home page has no information like affiliation or researcher fields, data collected from DBLP would give user a hint for what to present there. Once user decided to add those in his homepage, the action log would be stored in the demo schema. Since the recommendation would be used by different users, the separated schemas design between data source and data manipulate can help us keep the recommendation data clean and complete. 
The most challenging part is suggestion. The suggestion part works within all the other function parts for users¡¯ convenience. It gives auto-complete function for users to avoid wrong entries, helps user present information they cannot provide precisely, combine recommendation result with its supports for user¡¯ judgement. The challenging also occurs in similar suggestion function. Auto-complete need different language and software in different situation. For recommendation, it used HTTP request with JavaScript, for website action, it used SQL and JSP. 
**Assessment and Future Plan**
Even though I have tried my best, there are problems beneath. For the HTTP response JSON format file, there exists package to handle that, but I have built my owned parser for that. As new recommendations come, the parse won¡¯t fit in all situation in my neo4j based website. Another problem is that I did not find a smart way to present the suggestion between researchers¡¯ recommendation results and users. Right now we just selected the highest weighted papers between them. Since I have graduated in this semester, this might be the end of the journey for me.
**Reflection**
Even though I thought it would be easy to finish the 3rd part of the whole recommendation system, the website construction occupied all my semester without finished. It is lucky to work in a professional group to discuss different algorithms and try our best to find solutions to endless problem, and it gives me more chance to practice and eventually work out a product



